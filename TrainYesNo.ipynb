{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.python.keras.utils.data_utils import Sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "print(tf.__version__)\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel_YesNo(vocab_size, batchSize, maxlen):\n",
    "    module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\"\n",
    "    bert_layer = hub.KerasLayer(module_url, trainable=True)\n",
    "    \n",
    "    #Document\n",
    "    input_ids_Document = tf.keras.layers.Input(shape = (maxlen, ), dtype = tf.int32)\n",
    "    token_type_ids_Document = tf.keras.layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "    attention_mask_Document = tf.keras.layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "\n",
    "    pooled_output_Document, bertOutput_Document = bert_layer([input_ids_Document, attention_mask_Document, token_type_ids_Document])\n",
    "    dOut = pooled_output_Document\n",
    "    #dOut = tf.keras.layers.Dense(128, activation = 'relu')(dOut)\n",
    "    \n",
    "    #Question\n",
    "    input_ids_Question = tf.keras.layers.Input(shape = (maxlen, ), dtype = tf.int32)\n",
    "    token_type_ids_Question = tf.keras.layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "    attention_mask_Question = tf.keras.layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "\n",
    "    pooled_output_Question, bertOutput_Question = bert_layer([input_ids_Question, attention_mask_Question, token_type_ids_Question])\n",
    "    qOut = pooled_output_Question\n",
    "    #qOut = tf.keras.layers.Dense(128, activation = 'relu')(qOut)\n",
    "    \n",
    "    #Concat Layer\n",
    "    concat = tf.keras.layers.Concatenate()([dOut, qOut])\n",
    "    \n",
    "    denseLayer = tf.keras.layers.Dense(128, activation = 'relu')(concat)\n",
    "    #denseLayer = tf.keras.layers.Dense(256, activation = 'relu')(denseLayer)\n",
    "    #denseLayer = tf.keras.layers.Dense(256, activation = 'relu')(denseLayer)\n",
    "    #denseLayer = tf.keras.layers.Dense(128, activation = 'relu')(denseLayer)\n",
    "    denseLayer = tf.keras.layers.Flatten()(denseLayer)\n",
    "    denseLayer = tf.keras.layers.Dense(2)(denseLayer)\n",
    "    \n",
    "    model = tf.keras.Model(inputs = [input_ids_Document, token_type_ids_Document, attention_mask_Document, input_ids_Question, token_type_ids_Question, attention_mask_Question],\n",
    "                           outputs = denseLayer)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BatchSize = 8\n",
    "SeqLength = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainGenSeq_short_YesNo(tf.keras.utils.Sequence, ):\n",
    "    def __init__(self, batchSize, sentenceLength):\n",
    "        self.batchSize = batchSize\n",
    "        self.trainFiles = os.listdir('D:/Python/Datasets/v1.0/train/')\n",
    "        self.trainingSamples = 307372 * 2\n",
    "        self.sentenceLength = sentenceLength\n",
    "        \n",
    "        #Load Vocab\n",
    "        slow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        save_path = \"bert_base_uncased/\"\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        slow_tokenizer.save_pretrained(save_path)\n",
    "        self.tokenizer = BertTokenizer('vocab.txt', lowercase = True)\n",
    "        self.vocabSize = len(self.tokenizer.vocab)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(self.trainingSamples // self.batchSize)\n",
    "    \n",
    "    def getLen(self):\n",
    "        return int(self.trainingSamples // self.batchSize)\n",
    "    \n",
    "    def attentionMasks(self,input_dims):\n",
    "        return [int(id > 0) for id in input_dims]\n",
    "        \n",
    "    def inputDims(self, dims):\n",
    "        return pad_sequences([dims], maxlen = self.sentenceLength, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")[0]\n",
    "    \n",
    "    def encode_sentence(self, sentence):\n",
    "        sentence = sent_tokenize(sentence)\n",
    "        word_tokenized = [self.tokenizer.tokenize(s) for s in sentence]\n",
    "        ans = ['[CLS]']\n",
    "        for x in word_tokenized:\n",
    "            ans += x\n",
    "            ans.append('[SEP]')\n",
    "        \n",
    "        ans = self.tokenizer.convert_tokens_to_ids(ans)\n",
    "        padNum = (SeqLength - len(ans))\n",
    "        if padNum < 0:\n",
    "            padNum = 0\n",
    "        \n",
    "        return ans + ([0] * padNum)\n",
    "     \n",
    "    def __getitem__(self, _):\n",
    "        documentStack = np.array([])\n",
    "        questionStack = np.array([])\n",
    "        answerStack = np.array([])\n",
    "        \n",
    "        document_AttStack = np.array([])\n",
    "        question_AttStack = np.array([])\n",
    "\n",
    "        document_SegStack = np.array([])\n",
    "        question_SegStack = np.array([])\n",
    "\n",
    "        First = True\n",
    "        \n",
    "        for file in self.trainFiles:\n",
    "            for line in open('D:/Python/Datasets/v1.0/train/' + file):\n",
    "                file = json.loads(line)\n",
    "                #annotations\n",
    "                if file.get('annotations')[0].get('short_answers'):\n",
    "                    s_Start = file.get('annotations')[0].get('short_answers')[0].get('start_token')\n",
    "                    s_End = file.get('annotations')[0].get('short_answers')[0].get('end_token')\n",
    "                    l_Start = file.get('annotations')[0].get('long_answer').get('start_token')\n",
    "                    l_End = file.get('annotations')[0].get('long_answer').get('end_token')\n",
    "\n",
    "                    #Question and Title\n",
    "                    question = file.get('question_text')\n",
    "\n",
    "                    #document\n",
    "                    document = []\n",
    "                    for indexs in file.get('document_tokens')[l_Start:l_End]:\n",
    "                        if indexs.get('html_token') == False:\n",
    "                            document.append(indexs.get('token'))\n",
    "                    \n",
    "                    #Fake Document OR No document\n",
    "                    fake = []\n",
    "                    randomNumber = random.randint(7500, 9000)\n",
    "                    front = random.choice([True, False])\n",
    "                    \n",
    "                    if front:\n",
    "                        try:\n",
    "                            for indexs in range(max(0, l_Start - randomNumber), min(len(file.get('document_tokens')),l_End - randomNumber)):\n",
    "                                if file.get('document_tokens')[indexs].get('html_token') == False:\n",
    "                                    fake.append(file.get('document_tokens')[indexs].get('token'))\n",
    "                                else:\n",
    "                                    indexs -= 1\n",
    "                        except:\n",
    "                            for indexs in range(max(0, l_Start + randomNumber), min(len(file.get('document_tokens')),l_End + randomNumber)):\n",
    "                                if file.get('document_tokens')[indexs].get('html_token') == False:\n",
    "                                    fake.append(file.get('document_tokens')[indexs].get('token'))\n",
    "                                else:\n",
    "                                    indexs -= 1\n",
    "                    else:\n",
    "                        try:\n",
    "                            for indexs in range(max(0, l_Start + randomNumber), min(len(file.get('document_tokens')),l_End + randomNumber)):\n",
    "                                if file.get('document_tokens')[indexs].get('html_token') == False:\n",
    "                                    fake.append(file.get('document_tokens')[indexs].get('token'))\n",
    "                                else:\n",
    "                                    indexs -= 1\n",
    "                        except:\n",
    "                            for indexs in range(max(0, l_Start - randomNumber), min(len(file.get('document_tokens')),l_End - randomNumber)):\n",
    "                                if file.get('document_tokens')[indexs].get('html_token') == False:\n",
    "                                    fake.append(file.get('document_tokens')[indexs].get('token'))\n",
    "                                else:\n",
    "                                    indexs -= 1\n",
    "                    \n",
    "                    document = ' '.join(document)\n",
    "                    fake = ' '.join(document)\n",
    "                    \n",
    "                    document = self.encode_sentence(str(document))\n",
    "                    fake = self.encode_sentence(str(fake))\n",
    "                    question = self.encode_sentence(str(question))\n",
    "                    \n",
    "                    fake_AttentionMask = self.attentionMasks(fake)\n",
    "                    document_AttentionMask = self.attentionMasks(document)\n",
    "                    question_AttentionMask = self.attentionMasks(question)\n",
    "                    \n",
    "                    fake_SegID = [0 for _ in range(len(fake))]\n",
    "                    document_SegID = [0 for _ in range(len(document))]\n",
    "                    question_SegID = [0 for _ in range(len(question))]\n",
    "\n",
    "                    if First:\n",
    "                        #Document\n",
    "                        documentStack = np.array([document])\n",
    "                        documentStack = np.append(documentStack, np.array([fake]), axis = 0)\n",
    "                        document_AttStack = np.array([document_AttentionMask])\n",
    "                        document_AttStack = np.append(document_AttStack, np.array([fake_AttentionMask]), axis = 0)\n",
    "                        document_SegStack = np.array([document_SegID])\n",
    "                        document_SegStack = np.append(document_SegStack, np.array([fake_AttentionMask]), axis = 0)\n",
    "                        \n",
    "                        #Add Question Again\n",
    "                        questionStack = np.array([question])\n",
    "                        questionStack = np.append(questionStack, np.array([question]), axis = 0)\n",
    "                        \n",
    "                        question_AttStack = np.array([question_AttentionMask])\n",
    "                        question_AttStack = np.append(question_AttStack, np.array([question_AttentionMask]), axis = 0)\n",
    "                        question_SegStack = np.array([question_SegID])\n",
    "                        question_SegStack = np.append(question_SegStack, np.array([question_SegID]), axis = 0)\n",
    "                        \n",
    "                        #Add Answer\n",
    "                        answerStack = np.array([np.array([1,0])])\n",
    "                        answerStack = np.append(answerStack, np.array([np.array([0,1])]), axis = 0)\n",
    "                        \n",
    "                        First = False\n",
    "                    else:\n",
    "                        documentStack = np.append(documentStack, np.array([document]), axis = 0)\n",
    "                        documentStack = np.append(documentStack, np.array([fake]), axis = 0)\n",
    "                        questionStack = np.append(questionStack, np.array([question]), axis = 0)\n",
    "                        questionStack = np.append(questionStack, np.array([question]), axis = 0)\n",
    "                        answerStack = np.append(answerStack, np.array([np.array([1,0])]), axis = 0)\n",
    "                        answerStack = np.append(answerStack, np.array([np.array([0,1])]), axis = 0)\n",
    "                        \n",
    "                        #Attention Mask\n",
    "                        document_AttStack = np.append(document_AttStack, np.array([document_AttentionMask]), axis = 0)\n",
    "                        document_AttStack = np.append(document_AttStack, np.array([fake_AttentionMask]), axis = 0)\n",
    "                        question_AttStack = np.append(question_AttStack, np.array([question_AttentionMask]), axis = 0)\n",
    "                        question_AttStack = np.append(question_AttStack, np.array([question_AttentionMask]), axis = 0)\n",
    "                        \n",
    "                        #SegmentIDs\n",
    "                        document_SegStack = np.append(document_SegStack, np.array([document_AttentionMask]), axis = 0)\n",
    "                        document_SegStack = np.append(document_SegStack, np.array([fake_AttentionMask]), axis = 0)\n",
    "                        question_SegStack = np.append(question_SegStack, np.array([question_SegID]), axis = 0)\n",
    "                        question_SegStack = np.append(question_SegStack, np.array([question_SegID]), axis = 0)\n",
    "                \n",
    "                if documentStack.shape[0] == self.batchSize:\n",
    "                    documentStack = np.reshape(documentStack, (documentStack.shape[0], 1, documentStack.shape[1]))\n",
    "                    questionStack = np.reshape(questionStack, (questionStack.shape[0], 1, questionStack.shape[1]))\n",
    "                    answerStack = np.reshape(answerStack, (answerStack.shape[0], 1, answerStack.shape[1]))\n",
    "                    First = True\n",
    "\n",
    "                    #print(type(documentStack), type(questionStack), type(answerStack))\n",
    "                    return [np.squeeze(documentStack), np.squeeze(document_AttStack), np.squeeze(document_SegStack), \n",
    "                            np.squeeze(questionStack), np.squeeze(question_AttStack), np.squeeze(question_SegStack)], np.squeeze(answerStack)\n",
    "                    \n",
    "                    documentStack = None\n",
    "                    titleStack = None\n",
    "                    questionStack = None\n",
    "                    answerStack = None\n",
    "\n",
    "trainGen = trainGenSeq_short_YesNo(BatchSize, SeqLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           [(None, 5000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           [(None, 5000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           [(None, 5000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_19 (InputLayer)           [(None, 5000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           [(None, 5000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           [(None, 5000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_3 (KerasLayer)      [(None, 768), (None, 109482241   input_16[0][0]                   \n",
      "                                                                 input_18[0][0]                   \n",
      "                                                                 input_17[0][0]                   \n",
      "                                                                 input_19[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "                                                                 input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1536)         0           keras_layer_3[0][0]              \n",
      "                                                                 keras_layer_3[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          196736      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 128)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 2)            258         flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 109,679,235\n",
      "Trainable params: 109,679,234\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = createModel_YesNo(trainGen.vocabSize, BatchSize, SeqLength)\n",
    "model.summary()\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(), loss = 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "[_Derived_]RecvAsync is cancelled.\n\t [[{{node loss_2/mul}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-744faad67e37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainGen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainGen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetLen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    601\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m       \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1016\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1018\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1019\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1020\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3579\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3580\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3581\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3582\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCancelledError\u001b[0m: [_Derived_]RecvAsync is cancelled.\n\t [[{{node loss_2/mul}}]]"
     ]
    }
   ],
   "source": [
    "model.fit(trainGen, epochs = 2, steps_per_epoch = trainGen.getLen(),verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('Tensorflow2': conda)",
   "language": "python",
   "name": "python37364bittensorflow2conda366007bebd6145e889dc22f0db6fe8ac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
