{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.python.keras.utils.data_utils import Sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import backend as K\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import tensorflow_hub as hub\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "print(tf.__version__)\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tf.test.is_gpu_available(cuda_only = False, min_cuda_compute_capability = None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "        n_fine_tune_layers=10,\n",
    "        pooling=\"mean\",\n",
    "        bert_path=\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\",\n",
    "        **kwargs,):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_path = bert_path\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_path,\n",
    "            trainable=self.trainable,\n",
    "            name=\"{}_module\".format(self.name)\n",
    "        )\n",
    "        trainable_vars = self.bert.variables\n",
    "        \n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        if self.pooling == \"first\":\n",
    "            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "            trainable_layers = [\"pooler/dense\"]\n",
    "\n",
    "        elif self.pooling == \"mean\":\n",
    "            trainable_vars = [\n",
    "                var\n",
    "                for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "            ]\n",
    "            trainable_layers = []\n",
    "        else:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        if self.pooling == \"first\":\n",
    "            pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"pooled_output\"\n",
    "            ]\n",
    "        elif self.pooling == \"mean\":\n",
    "            result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "\n",
    "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
    "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
    "            input_mask = tf.cast(input_mask, tf.float32)\n",
    "            pooled = masked_reduce_mean(result, input_mask)\n",
    "        else:\n",
    "            raise NameError(f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\")\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel_YesNo(vocab_size, batchSize, maxlen):\n",
    "    bLayer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\", trainable=True)\n",
    "    \n",
    "    #Document\n",
    "    input_ids_Document = tf.keras.layers.Input(shape = (maxlen, ), dtype = tf.int32)\n",
    "    token_type_ids_Document = tf.keras.layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "    attention_mask_Document = tf.keras.layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "    \n",
    "    bertInputs_Document = [input_ids_Document, token_type_ids_Document, attention_mask_Document]\n",
    "    \n",
    "    #bertOutput_Document = bLayer(n_fine_tune_layers = 3)(bertInputs_Document)\n",
    "    bertOutput_Document, _ = bLayer(bertInputs_Document)\n",
    "    \n",
    "    #Question\n",
    "    input_ids_Question = tf.keras.layers.Input(shape = (maxlen, ), dtype = tf.int32)\n",
    "    token_type_ids_Question = tf.keras.layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "    attention_mask_Question = tf.keras.layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "    \n",
    "    bertInputs_Question = [input_ids_Question, token_type_ids_Question, attention_mask_Question]\n",
    "    \n",
    "    #bertInputs_Question = bLayer(n_fine_tune_layers = 3)(bertInputs_Question)\n",
    "    bertInputs_Question, _ = bLayer(bertInputs_Question)\n",
    "    \n",
    "    #Concat Layer\n",
    "    concat = tf.keras.layers.Concatenate()([bertOutput_Document, bertInputs_Question])\n",
    "    \n",
    "    denseLayer = tf.keras.layers.Dense(128, activation = 'relu')(concat)\n",
    "    denseLayer = tf.keras.layers.Flatten()(denseLayer)\n",
    "    denseLayer = tf.keras.layers.Dense(2)(denseLayer)\n",
    "    \n",
    "    model = tf.keras.Model(inputs = [input_ids_Document, token_type_ids_Document, attention_mask_Document, input_ids_Question, token_type_ids_Question, attention_mask_Question],\n",
    "                           outputs = [denseLayer])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "BatchSize = 16\n",
    "SeqLength = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainGenSeq_short_YesNo(tf.keras.utils.Sequence, ):\n",
    "    def __init__(self, batchSize, sentenceLength):\n",
    "        self.batchSize = batchSize\n",
    "        self.trainFiles = os.listdir('D:/Python/Datasets/v1.0/train/')\n",
    "        self.trainingSamples = 307372 * 2\n",
    "        self.sentenceLength = sentenceLength\n",
    "        \n",
    "        #Load Vocab\n",
    "        slow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        save_path = \"bert_base_uncased/\"\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        slow_tokenizer.save_pretrained(save_path)\n",
    "        self.tokenizer = BertTokenizer('vocab.txt', lowercase = True)\n",
    "        self.vocabSize = len(self.tokenizer.vocab)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(self.trainingSamples // self.batchSize)\n",
    "    \n",
    "    def getLen(self):\n",
    "        return int(self.trainingSamples // self.batchSize)\n",
    "    \n",
    "    def attentionMasks(self,input_dims):\n",
    "        return [int(id > 0) for id in input_dims]\n",
    "        \n",
    "    def inputDims(self, dims):\n",
    "        return pad_sequences([dims], maxlen = self.sentenceLength, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")[0]\n",
    "    \n",
    "    def encode_sentence(self, sentence):\n",
    "        sentence = sent_tokenize(sentence)\n",
    "        ans = []\n",
    "        for i in range(len(sentence)):\n",
    "            encode_sent = self.tokenizer.encode(sentence[i],add_special_tokens = True)\n",
    "            ans += encode_sent\n",
    "\n",
    "        ans = pad_sequences([ans], maxlen = self.sentenceLength, dtype = \"long\", value = 0, truncating = \"post\", padding = \"post\")\n",
    "        return ans[0]\n",
    "    \n",
    "    def __getitem__(self, _):\n",
    "        documentStack = np.array([])\n",
    "        questionStack = np.array([])\n",
    "        answerStack = np.array([])\n",
    "        \n",
    "        document_AttStack = np.array([])\n",
    "        question_AttStack = np.array([])\n",
    "\n",
    "        document_SegStack = np.array([])\n",
    "        question_SegStack = np.array([])\n",
    "\n",
    "        First = True\n",
    "        \n",
    "        for file in self.trainFiles:\n",
    "            for line in open('D:/Python/Datasets/v1.0/train/' + file):\n",
    "                file = json.loads(line)\n",
    "                #annotations\n",
    "                if file.get('annotations')[0].get('short_answers'):\n",
    "                    s_Start = file.get('annotations')[0].get('short_answers')[0].get('start_token')\n",
    "                    s_End = file.get('annotations')[0].get('short_answers')[0].get('end_token')\n",
    "                    l_Start = file.get('annotations')[0].get('long_answer').get('start_token')\n",
    "                    l_End = file.get('annotations')[0].get('long_answer').get('end_token')\n",
    "\n",
    "                    #Question and Title\n",
    "                    question = file.get('question_text')\n",
    "\n",
    "                    #document\n",
    "                    document = []\n",
    "                    for indexs in file.get('document_tokens')[l_Start:l_End]:\n",
    "                        if indexs.get('html_token') == False:\n",
    "                            document.append(indexs.get('token'))\n",
    "                    \n",
    "                    #Fake Document OR No document\n",
    "                    fake = []\n",
    "                    randomNumber = random.randint(7500, 9000)\n",
    "                    front = random.choice([True, False])\n",
    "                    \n",
    "                    if front:\n",
    "                        try:\n",
    "                            for indexs in range(max(0, l_Start - randomNumber), min(len(file.get('document_tokens')),l_End - randomNumber)):\n",
    "                                if file.get('document_tokens')[indexs].get('html_token') == False:\n",
    "                                    fake.append(file.get('document_tokens')[indexs].get('token'))\n",
    "                                else:\n",
    "                                    indexs -= 1\n",
    "                        except:\n",
    "                            for indexs in range(max(0, l_Start + randomNumber), min(len(file.get('document_tokens')),l_End + randomNumber)):\n",
    "                                if file.get('document_tokens')[indexs].get('html_token') == False:\n",
    "                                    fake.append(file.get('document_tokens')[indexs].get('token'))\n",
    "                                else:\n",
    "                                    indexs -= 1\n",
    "                    else:\n",
    "                        try:\n",
    "                            for indexs in range(max(0, l_Start + randomNumber), min(len(file.get('document_tokens')),l_End + randomNumber)):\n",
    "                                if file.get('document_tokens')[indexs].get('html_token') == False:\n",
    "                                    fake.append(file.get('document_tokens')[indexs].get('token'))\n",
    "                                else:\n",
    "                                    indexs -= 1\n",
    "                        except:\n",
    "                            for indexs in range(max(0, l_Start - randomNumber), min(len(file.get('document_tokens')),l_End - randomNumber)):\n",
    "                                if file.get('document_tokens')[indexs].get('html_token') == False:\n",
    "                                    fake.append(file.get('document_tokens')[indexs].get('token'))\n",
    "                                else:\n",
    "                                    indexs -= 1\n",
    "                    \n",
    "                    document = ' '.join(document)\n",
    "                    fake = ' '.join(document)\n",
    "\n",
    "                    document = self.encode_sentence(document)\n",
    "                    fake = self.encode_sentence(fake)\n",
    "                    question = self.encode_sentence(question)\n",
    "                    \n",
    "                    fake_AttentionMask = self.attentionMasks(fake)\n",
    "                    document_AttentionMask = self.attentionMasks(document)\n",
    "                    question_AttentionMask = self.attentionMasks(question)\n",
    "                    \n",
    "                    fake_SegID = [0 for _ in range(len(fake))]\n",
    "                    document_SegID = [0 for _ in range(len(document))]\n",
    "                    question_SegID = [0 for _ in range(len(question))]\n",
    "\n",
    "                    if First:\n",
    "                        #Document\n",
    "                        documentStack = np.array([document])\n",
    "                        documentStack = np.append(documentStack, np.array([fake]), axis = 0)\n",
    "                        document_AttStack = np.array([document_AttentionMask])\n",
    "                        document_AttStack = np.append(document_AttStack, np.array([fake_AttentionMask]), axis = 0)\n",
    "                        document_SegStack = np.array([document_SegID])\n",
    "                        document_SegStack = np.append(document_SegStack, np.array([fake_AttentionMask]), axis = 0)\n",
    "                        \n",
    "                        #Add Question Again\n",
    "                        questionStack = np.array([question])\n",
    "                        questionStack = np.append(questionStack, np.array([question]), axis = 0)\n",
    "                        \n",
    "                        question_AttStack = np.array([question_AttentionMask])\n",
    "                        question_AttStack = np.append(question_AttStack, np.array([question_AttentionMask]), axis = 0)\n",
    "                        question_SegStack = np.array([question_SegID])\n",
    "                        question_SegStack = np.append(question_SegStack, np.array([question_SegID]), axis = 0)\n",
    "                        \n",
    "                        #Add Answer\n",
    "                        answerStack = np.array([np.array([1,0])])\n",
    "                        answerStack = np.append(answerStack, np.array([np.array([0,1])]), axis = 0)\n",
    "                        \n",
    "                        First = False\n",
    "                    else:\n",
    "                        documentStack = np.append(documentStack, np.array([document]), axis = 0)\n",
    "                        documentStack = np.append(documentStack, np.array([fake]), axis = 0)\n",
    "                        questionStack = np.append(questionStack, np.array([question]), axis = 0)\n",
    "                        questionStack = np.append(questionStack, np.array([question]), axis = 0)\n",
    "                        answerStack = np.append(answerStack, np.array([np.array([1,0])]), axis = 0)\n",
    "                        answerStack = np.append(answerStack, np.array([np.array([0,1])]), axis = 0)\n",
    "                        \n",
    "                        #Attention Mask\n",
    "                        document_AttStack = np.append(document_AttStack, np.array([document_AttentionMask]), axis = 0)\n",
    "                        document_AttStack = np.append(document_AttStack, np.array([fake_AttentionMask]), axis = 0)\n",
    "                        question_AttStack = np.append(question_AttStack, np.array([question_AttentionMask]), axis = 0)\n",
    "                        question_AttStack = np.append(question_AttStack, np.array([question_AttentionMask]), axis = 0)\n",
    "                        \n",
    "                        #SegmentIDs\n",
    "                        document_SegStack = np.append(document_SegStack, np.array([document_AttentionMask]), axis = 0)\n",
    "                        document_SegStack = np.append(document_SegStack, np.array([fake_AttentionMask]), axis = 0)\n",
    "                        question_SegStack = np.append(question_SegStack, np.array([question_SegID]), axis = 0)\n",
    "                        question_SegStack = np.append(question_SegStack, np.array([question_SegID]), axis = 0)\n",
    "                \n",
    "                if documentStack.shape[0] == self.batchSize:\n",
    "                    documentStack = np.reshape(documentStack, (documentStack.shape[0], 1, documentStack.shape[1]))\n",
    "                    questionStack = np.reshape(questionStack, (questionStack.shape[0], 1, questionStack.shape[1]))\n",
    "                    answerStack = np.reshape(answerStack, (answerStack.shape[0], 1, answerStack.shape[1]))\n",
    "                    First = True\n",
    "\n",
    "                    #print(type(documentStack), type(questionStack), type(answerStack))\n",
    "                    return [np.squeeze(documentStack), np.squeeze(document_AttStack), np.squeeze(document_SegStack), \n",
    "                            np.squeeze(questionStack), np.squeeze(question_AttStack), np.squeeze(question_SegStack)], np.squeeze(answerStack)\n",
    "                    \n",
    "                    documentStack = None\n",
    "                    titleStack = None\n",
    "                    questionStack = None\n",
    "                    answerStack = None\n",
    "\n",
    "trainGen = trainGenSeq_short_YesNo(BatchSize, SeqLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_25 (InputLayer)           [(None, 10000)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_26 (InputLayer)           [(None, 10000)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_27 (InputLayer)           [(None, 10000)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_28 (InputLayer)           [(None, 10000)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_29 (InputLayer)           [(None, 10000)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_30 (InputLayer)           [(None, 10000)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_4 (KerasLayer)      [(None, 768), (None, 109482241   input_25[0][0]                   \n",
      "                                                                 input_26[0][0]                   \n",
      "                                                                 input_27[0][0]                   \n",
      "                                                                 input_28[0][0]                   \n",
      "                                                                 input_29[0][0]                   \n",
      "                                                                 input_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1536)         0           keras_layer_4[0][0]              \n",
      "                                                                 keras_layer_4[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 128)          196736      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 128)          0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 2)            258         flatten_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 109,679,235\n",
      "Trainable params: 109,679,234\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = createModel_YesNo(trainGen.vocabSize, BatchSize, SeqLength)\n",
    "model.summary()\n",
    "model.compile(optimizer = Adam(), loss = 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "[_Derived_]RecvAsync is cancelled.\n\t [[{{node loss_3/mul}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-402ab8549e42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainGen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainGen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetLen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    601\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m       \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1016\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1018\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1019\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1020\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3579\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3580\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3581\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3582\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCancelledError\u001b[0m: [_Derived_]RecvAsync is cancelled.\n\t [[{{node loss_3/mul}}]]"
     ]
    }
   ],
   "source": [
    "model.fit(trainGen, epochs = 10, steps_per_epoch = trainGen.getLen(),verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('Tensorflow2': conda)",
   "language": "python",
   "name": "python37364bittensorflow2conda366007bebd6145e889dc22f0db6fe8ac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
