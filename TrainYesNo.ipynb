{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.python.keras.utils.data_utils import Sequence\n",
    "\n",
    "import bert\n",
    "from bert import tokenization\n",
    "from bert import bert_tokenization\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel_YesNo(vocab_size, batchSize):\n",
    "    #Document\n",
    "    document_input = tf.keras.layers.Input(batch_shape = [batchSize, None])\n",
    "    document = tf.keras.layers.Embedding(vocab_size, 256, batch_input_shape = [batchSize, None])(document_input)\n",
    "    document = tf.keras.layers.GRU(1024, return_sequences = True, stateful = True, recurrent_initializer='glorot_uniform')(document)\n",
    "    document = tf.keras.layers.Dense(256, activation = 'relu')(document)\n",
    "    document_model = tf.keras.models.Model(inputs = document_input, outputs = document)\n",
    "    \n",
    "    #Question\n",
    "    question_input = tf.keras.layers.Input(batch_shape = [batchSize, None])\n",
    "    question = tf.keras.layers.Embedding(vocab_size, 256)(question_input)\n",
    "    question = tf.keras.layers.GRU(1024, return_sequences = True, stateful = True, recurrent_initializer='glorot_uniform')(question)\n",
    "    question = tf.keras.layers.Dense(126, activation = 'relu')(question)\n",
    "    question_model = tf.keras.models.Model(inputs = question_input, outputs = question)\n",
    "    \n",
    "    #concat\n",
    "    model = tf.keras.layers.concatenate([document_model.output, question_model.output])\n",
    "    model = tf.keras.layers.Dense(2, activation = 'softmax')(model)\n",
    "    \n",
    "    finalModel = tf.keras.models.Model(inputs = [document_input, question_input], outputs = model)\n",
    "    \n",
    "    return finalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainGenSeq_short_YesNo(tf.keras.utils.Sequence, ):\n",
    "    def __init__(self, batchSize, sentenceLength):\n",
    "        self.batchSize = batchSize\n",
    "        self.trainFiles = os.listdir('D:/Python/Datasets/v1.0/train/')\n",
    "        self.tokenizer = self.loadTokenizer()\n",
    "        self.trainingSamples = 307372\n",
    "        self.sentenceLength = sentenceLength\n",
    "        \n",
    "        #Load Vocab\n",
    "        self.tokenizer = bert_tokenization.FullTokenizer(vocab_file='D:/Python/Q_A/uncased_L-24_H-1024_A-16/vocab.txt', do_lower_case=True)\n",
    "        self.vocabSize = len(self.tokenizer.vocab)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(self.trainingSamples // self.batchSize)\n",
    "    \n",
    "    def getLen(self):\n",
    "        return int(self.trainingSamples // self.batchSize)\n",
    "    \n",
    "    def loadTokenizer(self):\n",
    "        myList = []\n",
    "        for line in open('vocab_word.txt'):\n",
    "            myList.append(line[:len(line)-1])\n",
    "        tk = tf.keras.preprocessing.text.Tokenizer(num_words=len(myList))\n",
    "        tk.fit_on_texts(myList)\n",
    "        return tk\n",
    "    \n",
    "    def encode_sentence(self, sentence):\n",
    "        ans = list(self.tokenizer.tokenize(sentence))\n",
    "        ans.append('[SEP]')\n",
    "        ans = self.tokenizer.convert_tokens_to_ids(ans)\n",
    "        ans = ans + ([0] * (self.sentenceLength - len(ans))) \n",
    "        return ans\n",
    "    \n",
    "    def __getitem__(self, _):\n",
    "        documentStack = np.array([])\n",
    "        questionStack = np.array([])\n",
    "        answerStack = np.array([])\n",
    "        First = True\n",
    "        \n",
    "        for file in self.trainFiles:\n",
    "            for line in open('D:/Python/Datasets/v1.0/train/' + file):\n",
    "                file = json.loads(line)\n",
    "                \n",
    "                #annotations\n",
    "                if file.get('annotations')[0].get('short_answers'):\n",
    "                    s_Start = file.get('annotations')[0].get('short_answers')[0].get('start_token')\n",
    "                    s_End = file.get('annotations')[0].get('short_answers')[0].get('end_token')\n",
    "                    l_Start = file.get('annotations')[0].get('long_answer').get('start_token')\n",
    "                    l_End = file.get('annotations')[0].get('long_answer').get('end_token')\n",
    "\n",
    "                    #Question and Title\n",
    "                    question = file.get('question_text')\n",
    "\n",
    "                    #document\n",
    "                    document = []\n",
    "                    for indexs in file.get('document_tokens')[l_Start:l_End]:\n",
    "                        if indexs.get('html_token') == False:\n",
    "                            document.append(indexs.get('token'))\n",
    "                    \n",
    "                    #Fake Document OR No document\n",
    "                    fake = []\n",
    "                    randomNumber = random.randint(int(0.75 * self.sentenceLength), self.sentenceLength)\n",
    "                    front = random.choice([True, False])\n",
    "                    \n",
    "                    if front:\n",
    "                        try:\n",
    "                            for indexs in range(max(0, l_Start - randomNumber), min(len(file.get('document_tokens')),l_End - randomNumber)):\n",
    "                                if file.get('document_tokens')[indexs].get('html_token') == False:\n",
    "                                    fake.append(file.get('document_tokens')[indexs].get('token'))\n",
    "                                else:\n",
    "                                    indexs -= 1\n",
    "                        except:\n",
    "                            for indexs in range(max(0, l_Start + randomNumber), min(len(file.get('document_tokens')),l_End + randomNumber)):\n",
    "                                if file.get('document_tokens')[indexs].get('html_token') == False:\n",
    "                                    fake.append(file.get('document_tokens')[indexs].get('token'))\n",
    "                                else:\n",
    "                                    indexs -= 1\n",
    "                    else:\n",
    "                        try:\n",
    "                            for indexs in range(max(0, l_Start + randomNumber), min(len(file.get('document_tokens')),l_End + randomNumber)):\n",
    "                                if file.get('document_tokens')[indexs].get('html_token') == False:\n",
    "                                    fake.append(file.get('document_tokens')[indexs].get('token'))\n",
    "                                else:\n",
    "                                    indexs -= 1\n",
    "                        except:\n",
    "                            for indexs in range(max(0, l_Start - randomNumber), min(len(file.get('document_tokens')),l_End - randomNumber)):\n",
    "                                if file.get('document_tokens')[indexs].get('html_token') == False:\n",
    "                                    fake.append(file.get('document_tokens')[indexs].get('token'))\n",
    "                                else:\n",
    "                                    indexs -= 1\n",
    "                    \n",
    "                    document = ' '.join(document)\n",
    "                    fake = ' '.join(document)\n",
    "\n",
    "                    document = self.encode_sentence(document)\n",
    "                    fake = self.encode_sentence(fake)\n",
    "                    question = self.encode_sentence(question)\n",
    "\n",
    "                    if First:\n",
    "                        documentStack = np.array([document])\n",
    "                        documentStack = np.append(documentStack, np.array([fake]), axis = 0)\n",
    "                        #Add Question Again\n",
    "                        questionStack = np.array([question])\n",
    "                        questionStack = np.append(questionStack, np.array([question]), axis = 0)\n",
    "                        #Add Answer\n",
    "                        answerStack = np.array([[1,0]])\n",
    "                        answerStack = np.append(answerStack, np.array([[0,1]]), axis = 0)\n",
    "                        First = False\n",
    "                    else:\n",
    "                        documentStack = np.append(documentStack, np.array([document]), axis = 0)\n",
    "                        documentStack = np.append(documentStack, np.array([fake]), axis = 0)\n",
    "                        questionStack = np.append(questionStack, np.array([question]), axis = 0)\n",
    "                        questionStack = np.append(questionStack, np.array([question]), axis = 0)\n",
    "                        answerStack = np.append(answerStack, np.array([[1,0]]), axis = 0)\n",
    "                        answerStack = np.append(answerStack, np.array([[0,1]]), axis = 0)\n",
    "                \n",
    "                if documentStack.shape[0] == self.batchSize:\n",
    "                    print(documentStack.shape, questionStack.shape, answerStack.shape)\n",
    "                    First = True\n",
    "\n",
    "                    #print(type(documentStack), type(questionStack), type(answerStack))\n",
    "                    yield [documentStack, questionStack], answerStack\n",
    "                    #return documentStack, answerStack\n",
    "                    \n",
    "                    documentStack = None\n",
    "                    titleStack = None\n",
    "                    questionStack = None\n",
    "                    #answerStack = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGen = trainGenSeq_short_YesNo(64, 5000)\n",
    "\n",
    "model = createModel_YesNo(trainGen.vocabSize, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 5000) (64, 5000) (64, 2)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-ab0c97ca73de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m model.fit(trainGen, epochs = 10, steps_per_epoch = trainGen.getLen(),\n\u001b[1;32m----> 2\u001b[1;33m                              verbose = 1)\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m           distribution_strategy=strategy)\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    604\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    607\u001b[0m   \u001b[1;31m# As a fallback for the data type that does not work with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m   \u001b[1;31m# _standardize_user_data, use the _prepare_model_with_inputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    611\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m     dataset = dataset_ops.DatasetV2.from_generator(generator, nested_dtypes,\n\u001b[1;32m--> 613\u001b[1;33m                                                    output_shapes=nested_shape)\n\u001b[0m\u001b[0;32m    614\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m       \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mfrom_generator\u001b[1;34m(generator, output_types, output_shapes, args)\u001b[0m\n\u001b[0;32m    538\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m       output_shapes = nest.map_structure_up_to(\n\u001b[1;32m--> 540\u001b[1;33m           output_types, tensor_shape.as_shape, output_shapes)\n\u001b[0m\u001b[0;32m    541\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m       \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\data\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure_up_to\u001b[1;34m(shallow_tree, func, *inputs)\u001b[0m\n\u001b[0;32m    469\u001b[0m                          for input_tree in inputs]\n\u001b[0;32m    470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m   \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtensors\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mall_flattened_up_to\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mpack_sequence_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshallow_tree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\data\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    469\u001b[0m                          for input_tree in inputs]\n\u001b[0;32m    470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m   \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtensors\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mall_flattened_up_to\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mpack_sequence_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshallow_tree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36mas_shape\u001b[1;34m(shape)\u001b[0m\n\u001b[0;32m   1214\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dims)\u001b[0m\n\u001b[0;32m    774\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;31m# Got a list of dimensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 776\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    777\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    774\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;31m# Got a list of dimensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 776\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    777\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36mas_dimension\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    716\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 718\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mDimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    719\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    191\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot convert %s to Dimension\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m       if (not isinstance(value, compat.bytes_or_text_types) and\n\u001b[0;32m    195\u001b[0m           self._value != value):\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'tuple'"
     ]
    }
   ],
   "source": [
    "model.fit(trainGen, epochs = 10, steps_per_epoch = trainGen.getLen(),verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('Tensorflow2': conda)",
   "language": "python",
   "name": "python37364bittensorflow2conda366007bebd6145e889dc22f0db6fe8ac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
